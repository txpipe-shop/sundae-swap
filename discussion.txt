--------------------------------------------
DISCUSSION ABOUT THE NEED FOR SingletonValue IN ORDER DATUMS

Quantumplation â€” 01/15/2024 5:58 PM
@francolq one thing I noticed over the weekend
is that we might be able to save more by removing pool_state, and just passing those fields directly down
as we do a destructure of the pool state / singleton value for each swap
seems like it could be a huge win, tbh; do you think it's worth it, or is that going to make the code way too nightmarish?

francolq â€” 01/15/2024 6:09 PM
I agree with that change
I was thinking about the type PoolState, without any strong conclusion yet, but some feelings

Quantumplation â€” 01/15/2024 6:14 PM
ok, i'm prototyping it on a branch right now

francolq â€” 01/15/2024 6:16 PM
something very related I wanted to ask you, is the need for SingletonValue instead of integer in several places
and in particular for quantity_a, quantity_b and quantity_lp in the PoolState
well, maybe specifically here is justified

francolq â€” 01/15/2024 6:25 PM
in the orders, I guess you need SingletonValue only when pool_ident is None, because when pool_ident is defined, all assets in all operations are clearly defined, am I right?

Quantumplation â€” 01/15/2024 6:38 PM
we need to make sure that the asset IDs are the ones we're swaping for; i.e. if I swap 100 ADA, i'm not swapping 100 ADA for "any token"; i'm swapping it specifically for RBERRY.

So I need to know the asset ID and the amounts. and SingletonValue was a convenient way to bundle them
I'm unpacking that into parameters, though leaving it for the order datum

francolq â€” 01/15/2024 6:52 PM
if you place the swap order on a specific pool (i.e. with a defined pool_ident) there is no need for SingletonValue as the pool has already a fixed pair.
so I understand you need SingletonValue when you don't specifiy the pool in the order 
that's what I understand so far

Quantumplation â€” 01/15/2024 6:55 PM
oh right, i read your message backwards; I thought you were saying you don't need it when pool_ident is none
but yea; also we're probably hitting diminishing returns for number of orders processed

francolq â€” 01/15/2024 6:56 PM
ok, and is it important for you to have this "open" orders that can be processed by any pool, right?
well, not "by any" but "by any that is able to"

Quantumplation â€” 01/15/2024 7:03 PM
yes, I believe so; It lets use implement a feature whereby the user can let the scooper route the order to an acceptable pool if it's open
for example, if one pool is super congested, some users might be ok with a worse price, but to skip the queue

--------------------------------------------
UNFINISHED DISCUSSION ABOUT AN IDEA TAKEN FROM MINSWAP CODE

Quantumplation â€” 01/17/2024 6:14 PM
@francolq was reading through minswaps open source contracts, and they use a trick where they count the number of scripts by checking the redeemers, instead of looping over the inputs
Does that work for us too?
or maybe not, because it will include the staking script, and the minting policy too, right?

francolq â€” 01/17/2024 6:21 PM
not sure, we can take a look, can you link me to minswap code?

Quantumplation â€” 01/17/2024 8:13 PM
https://github.com/minswap/minswap-stableswap/blob/main/validators/pool_validator.ak#L309-L310

--------------------------------------------
DISCUSSION ABOUT ASSUMPTIONS ON THE ORDERING OF REFERENCE INPUTS

francolq â€” 01/24/2024 3:47 PM
hello @Quantumplation ! We are taking a look at the usage of reference inputs. We see you are assuming that the settings UTxO is the first ref input, always before the ref inputs for the order and pool scripts.
How do you ensure this? We understand it can be done at start with some "farming" but @ignacio.dopazo  detected that if there is a settings update, the settings UTxO will change and it may stop being the first one lexicographically.
Did you contemplate this possiblity? I guess it means that the "farming" should also be done on every settings update

Quantumplation â€” 01/24/2024 3:57 PM
yea, that's our current plan; though we'd do it the other way: farm the reference scripts so that they had very "high" tx hashes, like starting with ffff, so we'd be able to update the settings more easily
I can describe this in our whitepaper / comments to make it clear 

francolq â€” 01/24/2024 4:19 PM
ok! yes, in my opinion it deserves a comment somewhere in the documentation

Quantumplation â€” 01/24/2024 4:19 PM
will do ðŸ™‚ thanks for highlighting

Quantumplation â€” 01/24/2024 5:36 PM
@francolq another way to address this is to just pay out the settings UTXO and all the reference scripts in the same transaction, so that output 0 is the settings UTXO, output 1+ are the reference scripts.

In this way, we can control the order they're in, without farming, and if we update the settings UTXO, we just spend the script references at the same time.

francolq â€” 01/24/2024 5:37 PM
in that case you will need a validator for the spending of the script ref UTxOs, right?

Quantumplation â€” 01/24/2024 5:38 PM
yes, but those will likely just be held in a wallet; they're not sensitive, since any of the scoopers could publish their own script references if we ever "went rogue"

francolq â€” 01/24/2024 5:38 PM
I was thinking you can just stop assuming it is first ref input and add an index for it in the redeemer

Quantumplation â€” 01/24/2024 5:39 PM
maybe, but that involves a recursion, increases the size of the redeemer, etc.

francolq â€” 01/24/2024 5:40 PM
yes, well as long as it is well documented, I think it is ok as it is right now

Quantumplation â€” 01/24/2024 5:41 PM
it's also not something we're doing for every single order, so it isn't a big performance hit if we have to change it
but if there's not a security concern, then it's nice to save a tiny bit ðŸ˜…

Quantumplation â€” 01/24/2024 5:43 PM
this also relies on scripts being small enough to fit in one transaction though heh

francolq â€” 01/24/2024 5:43 PM
true

--------------------------------------------
TECHNICAL DISCUSSION ABOUT USING LISTS OF PAIRS IN AIKEN

Quantumplation â€” 01/24/2024 4:51 PM
btw @francolq another small tweak we have to make: https://github.com/SundaeSwap-finance/sundae-contracts/pull/40

Basically, the haskell / plutus / aiken serialization for lists of tuples gets serialized as a map; but the cbor spec technically forbids maps from relying on the ordering of the keys; so most default serializers across different languages end up sorting the map key order, and this would screw up the execution order of things
in theory, I wish Aiken would just treat List<(a,b)> as lists, and use Dict<...> or AssocList<...> for things that serialize as maps, using those in the script context where appropriate
cause this is a pretty subtle gotcha; even after 7 months of extensive testing (5500 orders processed on preview so far), we didn't run into it until just yesterday

francolq â€” 01/24/2024 4:54 PM
oh ok, I see you had to introduce this new type InputOrderItem
did you measure the impact in terms of mem/cpu?

Quantumplation â€” 01/24/2024 4:54 PM
not yet
@ruko is going to do that soon, I think

francolq â€” 01/24/2024 4:55 PM
ok! just curious about it

Quantumplation â€” 01/24/2024 4:55 PM
it should be very similar, because it's the same number of destructuring, etc.; it makes the cbor slightly bigger, but I don't think that'll have a huge impact

francolq â€” 01/24/2024 4:56 PM
hope so

francolq â€” 01/24/2024 5:01 PM
sorry, I have a question, do you mean that something like [(2, None), (1, None), (3, None)] could be converted into [(1, None), (2, None), (3, None)] after serialization/deserialization?

Quantumplation â€” 01/24/2024 5:05 PM
yes, depending on the language the serializer is in, if you rely on the default cbor serializer.

For example, to get the go CBOR serializer to serialize this list as a map, we had to express it as a map[int]execution in our go code; and even if we inserted things as (2,1,3), it'd end up serializing as (1,2,3).

This never came up for us, because the way we were doing load tests just happened to always produce orders in increasing order (we'd process a batch all from the same txHash with incrementing output numbers).  But as soon as we had something that needed to go out of order, it started failing.

If we made that field an array, it would serialize as an array, and aiken wouldn't be able to parse it
because aiken requires that List<(a,b)> be a map
so we'd have had to end up writing a custom serializer in go
so if this impacts our throughput meaningfully, we might still go that route

francolq â€” 01/24/2024 5:07 PM
thanks, honestly I didn't know Aiken was doing this

Quantumplation â€” 01/24/2024 5:07 PM
yea, it's really subtle, make sure you add it to the list of things to consider during audits ðŸ˜…
another approach we considered was changing the type to List<(a,b,())>; a three-tuple won't serialize this way, I believe, and the () might be lower overhead
but not sure

Quantumplation â€” 01/24/2024 8:44 PM
turns out it hurt us enough to drop us down by one order max size, so we tried out the other approach here:
https://github.com/SundaeSwap-finance/sundae-contracts/pull/43

This is slightly faster than the 2-tuple version, strangely enough

francolq â€” 01/24/2024 9:26 PM
Interesting, maybe the 2-tuple version has an overhead because of the map encoding

Quantumplation â€” 01/24/2024 9:32 PM
yep I think so

--------------------------------------------
DISCUSSION ABOUT OPTIMIZING THE INDEXING OF THE INPUTS LIST

francolq â€” 01/24/2024 5:43 PM
the conversation about the input_order list is driving me to a second question related to the optimizations you did in process_orders for obtaining the inputs (remaining_inputs parameter, etc.)
I guess in a realistic scenario it is to be expected that the inputs are randomly sorted, right?  could you verify experimentally that these optimizations help in randomly sorted inputs?

Quantumplation â€” 01/24/2024 6:11 PM
I thought I read some research on this, but I can't find it now; just wrote a quick empirical test for it, that counted:
the length of the longest "monotonic slice"
the sum of (length of each monotonic slice - 1); i.e. the number of times we'll step forward with remaining_inputs instead of all_inputs
the number of saved builtin.tail_lists (i.e. if we had started over from all_inputs and skiped to the jth index, how many wasted skips would we have performed)

Then prints out the avg, min, and max. Please double check my implementation though, it's a bit subtle to think through.

function shuffle(array) {
  let currentIndex = array.length,  randomIndex;

  // While there remain elements to shuffle.
  while (currentIndex > 0) {

    // Pick a remaining element.
    randomIndex = Math.floor(Math.random() * currentIndex);
    currentIndex--;

    // And swap it with the current element.
    [array[currentIndex], array[randomIndex]] = [
      array[randomIndex], array[currentIndex]];
  }

  return array;
}

let longest_consecutive_sequences = [];
let sum_consecutive_sequences = [];
let saved_recursions = [];

for(let i = 0; i < 1000000; i++) {
  let sequence = Array(40).fill().map((_, index) => index)
  shuffle(sequence)
  let longest_consecutive_sequence = 1;
  let current_consecutive_sequence = 1;
  let sum_consecutive_sequence = 0;
  let prev_element = -1;
  let saved_rec = 0;
  for(let j = 0; j < sequence.length; j++) {
    let j_elem = sequence[j];
    if(j_elem < prev_element) {
      if(current_consecutive_sequence > longest_consecutive_sequence) {
        longest_consecutive_sequence = current_consecutive_sequence
      }
      if (current_consecutive_sequence > 1) {
        sum_consecutive_sequence += current_consecutive_sequence - 1
      }
      current_consecutive_sequence = 1;
      prev_element = j_elem;
      continue
    } else {
      current_consecutive_sequence += 1;
      prev_element = j_elem;
      saved_rec += j
    }
  }
  saved_recursions.push(saved_rec)
  sum_consecutive_sequences.push(sum_consecutive_sequence)
  longest_consecutive_sequences.push(longest_consecutive_sequence);
}
console.log("avg seq", longest_consecutive_sequences.reduce((a, b) => a + b, 0) / longest_consecutive_sequences.length)
console.log("min seq", longest_consecutive_sequences.reduce((a, b) => a < b ? a : b, 999))
console.log("max seq", longest_consecutive_sequences.reduce((a, b) => a > b ? a : b, 0))

console.log("avg sum", sum_consecutive_sequences.reduce((a, b) => a + b, 0) / sum_consecutive_sequences.length)
console.log("min sum", sum_consecutive_sequences.reduce((a, b) => a < b ? a : b, 999))
console.log("max sum", sum_consecutive_sequences.reduce((a, b) => a > b ? a : b, 0))

console.log("avg saved", saved_recursions.reduce((a, b) => a + b, 0) / saved_recursions.length)
console.log("min saved", saved_recursions.reduce((a, b) => a < b ? a : b, 99999999))
console.log("max saved", saved_recursions.reduce((a, b) => a > b ? a : b, 0))


Across a million runs, this gives:
avg seq 4.079384
min seq 2
max seq 11
avg sum 19.784143
min sum 11
max sum 29
avg saved 390.033646
min saved 178
max saved 591


meaning, on average, we save 390 builtin.tail_lists, I think.
I get similar results for batch sizes of size 37 too
And if the protocol parameters increase slightly and we can fit 50 orders, we save an average of 612 skips
good to put these stats in the whitepaper, if you confirm my implementation is correct

francolq â€” 01/30/2024 9:20 PM
I am thinking about an alternative to the remaining_inputs optimization. The goal is to have a very fast way to index the all_inputs list. The ideal solution would be to build a binary search tree (lot of work, not sure if it will work in practice). A simpler solution that is easy to try is to have a list with the 2nd half of the inputs list, say "snd_half_inputs" (instead of remaining_inputs). When looking for the input, then index on "all_inputs" if it is in the 1st half, or index on "snd_half_inputs" if it is in the 2nd half.

Quantumplation â€” 01/30/2024 9:30 PM
at this point i'm less concerned with micro-optimizations; If there's an easy way to get a big win, for sure, but i'm not sure if this would save over the existing contracts, and 37 orders is already a big improvement

francolq â€” 01/30/2024 10:11 PM
Yes, sure, sometimes I can't help myself with optimizations ðŸ™‚ This is one of the last ones. In this case I think it would also contribute to code simplicity,  so I think it is worth a shot.

Quantumplation â€” 01/30/2024 10:34 PM
how would you find "half the list"? just an initial scan first?

francolq â€” 01/30/2024 11:18 PM
I think a call to unsafe_fast_index_skip_with_tail(inputs, total_inputs / 2)
will return the second half

Quantumplation â€” 01/31/2024 12:36 AM
We don't have a count of inputs, do we?

francolq â€” 01/31/2024 1:34 AM
hmm no, but I guess it can be done together with count_orders

francolq â€” 01/31/2024 4:03 PM
@Quantumplation I could do a quick test of the "half list" idea
with the original code:
    â”‚ PASS [mem: 17142175, cpu: 6699341695] process_30_shuffled_orders_test
with the half list:
    â”‚ PASS [mem: 17072969, cpu: 6676825710] process_30_shuffled_orders_test
I can push that in an "experimental" branch if you want to take a look

Quantumplation â€” 01/31/2024 4:05 PM
so we save about 0.4% of the max budget ðŸ˜…
on memory anyway
(17142175 - 17072969) / 14000000

francolq â€” 01/31/2024 4:06 PM
yes, but I think something good is that it is stable, it is not sensitive to the shuffling of the orders
for n orders it always saves (n / 2)^2  tail operations

Quantumplation â€” 01/31/2024 4:07 PM
how does it impact things if the orders are in sorted order?

francolq â€” 01/31/2024 4:07 PM
let me see
â”‚ PASS [mem: 16752165, cpu: 6548794822] process_30_shuffled_orders_test
this is in sorted order for the original code (with remaining_inputs)

francolq â€” 01/31/2024 4:14 PM
the half list optimization is the same as before:
    â”‚ PASS [mem: 17072969, cpu: 6676825710] process_30_shuffled_orders_test
if the expectation is that the input_orders is sorted, the remaining_inputs approach is more efficient

Quantumplation â€” 01/31/2024 4:17 PM
so:
in the average case, your solution is about 0.4% more efficient (not enough to get us an extra order)
in the optimal case, our solution is about 2.2% more efficient (likely enough for an extra order or two)

So while most of the time the orders will be shuffled, when we can sort them (i.e. all of the pre-launch orders from a given token, or if the DAO votes to relax the FCFS requirement under load), we can fit a few more orders and burn through that load faster

francolq â€” 01/31/2024 4:20 PM
out of curiosity, this is the worst case, a reversed input_orders list:
    â”‚ PASS [mem: 17251431, cpu: 6735306647] process_30_shuffled_orders_test

Quantumplation â€” 01/31/2024 4:22 PM
for which, our impl or yours?

francolq â€” 01/31/2024 4:22 PM
for yours, with remaining_inputs
with half_list it is always the same
it is insensitive to the sorting order
I think it is ok, you have strong arguments for your version!
as a side comment, Lucas shared me this binary search tree, you may find it interesting:
https://github.com/aiken-lang/trees/blob/main/lib/aiken/trees/bst.ak

Quantumplation â€” 01/31/2024 4:26 PM
nods my guess is that the sizes of txs are going to be such that the asymptotics never make such a tree worth it, but it's good to have an implementation heh

francolq â€” 01/31/2024 4:27 PM
I agree ðŸ™‚

francolq â€” 02/01/2024 5:50 PM
@Quantumplation we realized with Nacho that we were using different versions of Aiken and getting very different mem/cpu budgets.
the test for the 30 orders is actually given more reasonable values with a more recent version of Aiken (v1.0.21-alpha+4b04517):
    â”‚ PASS [mem: 12571807, cpu: 4883611979] process_30_shuffled_orders_test

Quantumplation â€” 02/01/2024 5:51 PM
ooo! nice!

francolq â€” 02/01/2024 5:51 PM
not 17 millions as I reported previoulsy with  v1.0.14-alpha
for the record, last version v1.0.24-alpha is worse than 1.0.21:
    â”‚ PASS [mem: 13673721, cpu: 5235633714] process_30_shuffled_orders_test

--------------------------------------------
DISCUSSION ABOUT COUNTING ORDERS LOGIC AND DOUBLE SATISFACTION (SEE ALSO: https://github.com/SundaeSwap-finance/sundae-contracts/issues/47)


Quantumplation â€” 01/29/2024 8:52 PM
@francolq responded to your github issue, and I disagree
I think the name count_orders is actually correct, and important for clarity
i.e. the consuming code cares about the count of orders; it doesn't care about the count of script inputs. So if you're reviewing a block of code for correctness, it better expresses the intent of the code
it's just an implementation detail that an efficient way to count the number of orders is to count the script inputs minus 1

francolq â€” 01/30/2024 1:11 AM
hello Pi! For me it is not an implementation detail that the function counts script inputs - 1, because the result will be different if there are extra script inputs that are not orders or the pool itself. When reading the code, I was wondering if the scooper was allowed to do this or not, because extra script inputs may enable attacks such as double satisfaction (maybe with another pool input). With the name "count_orders" I thought at first that these extra script inputs were not being counted. But then reading the implementation I found that they are actually counted, meaning that the check Â´expect simple_count + strategy_count == real_order_countÂ´ not only implies that there is no under-reporting but also that there are no extra "unrelated" script inputs. 

Quantumplation â€” 01/30/2024 1:27 AM
well, that's because you're reading it as an auditor.

From the meaning of the code, we use count_orders to calculate the amortized script fee. The number of script inputs is meaningless to that value, we're not splitting the base fee among every script input, we're splitting it among every order.

The "correct" way to implement count_orders would be to traverse over each of the inputs, and check if it has the order datum attached; it just so happens that that will always (in our scripts) be equivalent to the number of script inputs minus 1; and in fact, the script will fail for other reasons if they are different.

So it's and important property, for sure, especially to an auditor checking correctness of the code, which is why there's like, a full dissertation in the comments of the function ðŸ˜…

But to someone trying to understand why the contracts do what they do, IMO count_orders is a lot clearer and true to the intention of the function

francolq â€” 01/30/2024 8:09 PM
hello @Quantumplation !! all ok with issue #47, time to move on to other stuff

--------------------------------------------
DISCUSSION ABOUT SYMMETRY OF DEPOSIT OPERATION (FINDING SSW-311)

Quantumplation â€” 01/31/2024 1:00 PM
I think we have a finding of our own
https://preview.cexplorer.io/tx/4cd7da636f59b536ea21ef538ac7455d27f4f0a2cba48f4287e4737c8d2358fa

In this transaction, we try to deposit 171 lovelace and 94 RBERRY; and it returns 171 lovelace to the user, and keeps the 94 rberry, without minting any LP tokens

Quantumplation â€” 01/31/2024 2:41 PM
ultimately it's really minor, because someone effectively just donated 171 lovelace worth of value to the pool
but still something we probably want to prevent

francolq â€” 02/01/2024 5:58 PM
so @Quantumplation I want to talk about deposit.ak and the finding you had with transaction 4cd7da636f59b536ea21ef538ac7455d27f4f0a2cba48f4287e4737c8d2358fa

Quantumplation â€” 02/01/2024 6:02 PM
yea! basically it's possible that the amounts are so low that when it computes the quantity of LP tokens, it comes up with 0; and so it distributes 0 LP tokens, and pays the 171 lovelace into the pool, and 0 B into the pool

francolq â€” 02/01/2024 6:02 PM
the result you had was to deposit (0, 94) liquidity (ADA, RBERRY),  and minted 0 LP tokens

Quantumplation â€” 02/01/2024 6:03 PM
so i think we just need an expect lp > 0 

francolq â€” 02/01/2024 6:03 PM
I was taking a look at the math
and comparing with some formulas we have from older projects
in your formulas you are using integer division
if you change these divisions to round up instead of down
you will have the following result: deposit (1, 94) liquidity (ADA, RBERRY),  and mint 34 LP tokens

Quantumplation â€” 02/01/2024 6:05 PM
does that risk draining the pool though? we'd be giving slightly more LP tokens than the value that you deposited, so if you then withdrew immediately, you'd get back slightly more than you put in
-- similar to the balancer formula
  doDeposit ret (DepositSingle coin amt) a b liq cons es =
    let
      de = denominator swapFees * 2
      nu = numerator swapFees
      diff = de - nu
      inPool CoinA = a
      inPool CoinB = b
      !liq2 = liq * liq
      !extraLiquidityTokens =
        unsafeSqrt
          (fromInteger liq2 + (liq2 * amt * diff) % (inPool coin * de)) -
        liq
      !liq_incr = liq + extraLiquidityTokens
      !liqABL = ofLiquidity extraLiquidityTokens
    in
      case coin of
        CoinA ->
          go (a + amt) b liq_incr ((ret, liqABL) : cons) es
        CoinB ->
          go a (b + amt) liq_incr ((ret, liqABL) : cons) es
  doDeposit ret (DepositMixed userGives) a b liq cons es =
    let !bInUnitsOfA = (userGives $$ CoinB * a) `divide` b
        !giveCoinA = userGives $$ CoinA
        !change =
          if bInUnitsOfA > giveCoinA then
            ((b * (bInUnitsOfA - giveCoinA)) `divide` a) `ofCoin` CoinB
          else
            (giveCoinA - bInUnitsOfA) `ofCoin` CoinA
        !userDeposits = noLiquidity (userGives - change)
        !extraLiquidityTokens = (userDeposits $$ CoinA * liq) `divide` a
        !output = ofLiquidity extraLiquidityTokens + noLiquidity change
    in
      go (a + userDeposits $$ CoinA) (b + userDeposits $$ CoinB) (liq + extraLiquidityTokens) ((ret, output) : cons) es

  doWithdrawal ret givesLiquidity a b liq cons es =
    let
      inPool CoinA = a
      inPool CoinB = b
      !withdrawn = noLiquidity $ memo \coin -> (givesLiquidity * inPool coin) `divide` liq
    in go (a - withdrawn $$ CoinA) (b - withdrawn $$ CoinB) (liq - givesLiquidity) ((ret, withdrawn) : cons) es
these are our v1 formulas that were audited by runtime verification
ignoring the DepositSingle case
EscrowSwap coin gives minTakes ->
      let
        !de = denominator swapFees
        !nu = numerator swapFees
        !diff = de - nu
      in
        case coin of
          CoinA
            | let !takes = (b * gives * diff) `divide` (a * de + gives * diff)
            , b > takes
            , Just takes >= minTakes ->
            go (a + gives) (b - takes) liq ((ret, takes `ofCoin` CoinB) : cons) es
          CoinB
            | let !takes = (a * gives * diff) `divide` (b * de + gives * diff)
            , a > takes
            , Just takes >= minTakes ->
            go (a - takes) (b + gives) liq ((ret, takes `ofCoin` CoinA) : cons) es
          _ -> error ()


And here's our swap, fwiw

francolq â€” 02/01/2024 6:06 PM
well, I am not sure but I don't think so, because it requires so many orders that the associated costs are not worth it

Quantumplation â€” 02/01/2024 6:07 PM
Runtime Verification do a lot of formal modelling and stuff, and helped us identify really subtle corner cases that avoid risks to the pool draining over time etc; so i'm very hesitant to deviate from that ðŸ˜…

francolq â€” 02/01/2024 6:07 PM
no problem

Quantumplation â€” 02/01/2024 6:08 PM
so for example, failing if the liquidity tokens are 0, since the amounts are so small, seems reasonable too

francolq â€” 02/01/2024 6:08 PM
yes, sure

Quantumplation â€” 02/01/2024 6:09 PM
that being said, if you do come across any math that is wrong / can be improved, i'm open to it
just want to be really solid in understanding exactly what the risks / effects of the change are

francolq â€” 02/01/2024 6:09 PM
yes, I will think more deeply about it

francolq â€” 02/02/2024 5:46 PM
hi @Quantumplation I was doing more tests with deposit and I have a question
do you expect all these operations to be symmetric in terms of the assets (A, B)?

Quantumplation â€” 02/02/2024 5:47 PM
what do you mean?

francolq â€” 02/02/2024 5:47 PM
I mean if you flip all the amounts and do the operation, the result show be the same
for instance in the case you found, you deposit 171 ADA and 94 RBERRY
and get back alla 171 ADA as change and 0 LPs
say I flip everything, all amounts
deposit 94 ADA and 171 RBERRY

Quantumplation â€” 02/02/2024 5:48 PM
no, i don't think i would expect that to be the same. because that's not going to represent the same ratio of assets

francolq â€” 02/02/2024 5:48 PM
(and of course flip amounts in the pool)

Quantumplation â€” 02/02/2024 5:49 PM
oh, then, perhaps

francolq â€” 02/02/2024 5:49 PM
the case is, as it is now, result is different
for this case in particular, 2 LPs are minted instead of 0

  let issued_lp_tokens =
    deposited_a * pool_state.quantity_lp.3rd / pool_state.quantity_a.3rd
and actual deposited amounts are (94, 1), not (94, 0) as expected by symmetry

Quantumplation â€” 02/02/2024 5:51 PM
I'm not sure.

Here's the goal of deposit:
accept, into the pool, quantities (a, b), such that, a and b are in the same ratio as reserve_a and reserve_b
the quantity of issued LP tokens, as a percentage of the new circulating supply, represents the same percentage that a or b have to the new reserve_a and reserve_b
(a, b) are at most the values specified in the datum
any surplus of either a or b is returned to the user
any rounding is in favor of more assets into the pool, to prevent subtle rounding attacks that drain the pool over time
 
francolq â€” 02/02/2024 5:53 PM
yes, I agree with the goals

Quantumplation â€” 02/02/2024 5:53 PM
so, the last bullet point might change symmetry by up to 1 unit of an asset

francolq â€” 02/02/2024 5:54 PM
but the rounding issues are kind of "underspecified" and several possible behaviours are acceptable

Quantumplation â€” 02/02/2024 5:54 PM
if you want to make a case for any particular one, i'm happy to hear it out / implement it

francolq â€” 02/02/2024 5:54 PM
I will, I think I almost have it
what I am pretty sure right now is that the asymmetry is true for deposit

francolq â€” 02/02/2024 5:56 PM
so my question somehow is that if you are willing to accept that for the exact same operation the results may be different depending on the asset order: (A, B) or (B, A)

Quantumplation â€” 02/02/2024 5:58 PM
It's hard for me to say, tbh. If it achieves the goals above, but is different by a few millionths of an ADA in a way that isn't going to let you amplify that difference and gain an advantage, then I'm fine with it being asymmetric. In particular, the order of assets is enforced by the contract, and 100 ADA / 50 RBERRY is a very different pool than 50 ADA / 100 RBERRY
i.e. the symmetry itself is an unimportant property, unless it's needed to ensure any of the above

francolq â€” 02/02/2024 6:01 PM
ok, I will summarize the finding, I think it is relevant to have it documented, it is not necessarily something that must be fixed
also, that (at least in this corner case) it is not the same computing the LP tokens from A or from B:
  let issued_lp_tokens = deposited_a * pool_state.quantity_lp.3rd / pool_state.quantity_a.3rd

is different than 
  let issued_lp_tokens = deposited_b * pool_state.quantity_lp.3rd / pool_state.quantity_b.3rd
in one case you get 0 (deposited_a is 0), in the other you get 2

Quantumplation â€” 02/02/2024 6:09 PM
correct, subject to rounding

francolq â€” 02/05/2024 7:13 PM
hi @Quantumplation ! I just pushed finding SSW-311, and last friday SSW-310, both about the deposit operation 
SSW-311 also includes a branch I pushed: https://github.com/SundaeSwap-finance/sundae-contracts/tree/francolq/ssw-311
with a couple of tests
I didn't do a PR because I am not sure if you want it merged
